{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d095a2a3-2f4f-49c4-a4c3-4f4306e3c788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 01:40:54.605699: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-18 01:40:56.200816: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, RandomSampler, SequentialSampler\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6d4efb7-237a-47ad-98fb-e8f5376b01d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.labels = dataset['label']\n",
    "        self.input_ids = dataset['input_ids']\n",
    "        self.attention_mask = dataset['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        input_ids = self.input_ids[idx]\n",
    "        attention_mask = self.attention_mask[idx]\n",
    "        return {\"label\": label, \"input_ids\":input_ids, \"attention_mask\":attention_mask}\n",
    "    \n",
    "\n",
    "def load_data(trainset_file_path, testset_file_path):\n",
    "    train_data = pd.read_csv(trainset_file_path, encoding='utf-8')\n",
    "    test_data = pd.read_csv(testset_file_path, encoding='utf-8')\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "def preprocessing_data(dataset, tokenizer, max_length):\n",
    "    concat_entity = []\n",
    "    for title, body in zip(dataset['Headline'], dataset['Content']):\n",
    "        total = title + '[SEP]' + body\n",
    "        concat_entity.append(total)\n",
    "        \n",
    "    tokenized_senteneces = tokenizer(\n",
    "        concat_entity,\n",
    "        return_tensors = \"pt\",\n",
    "        padding = True,\n",
    "        truncation = True,\n",
    "        max_length = max_length,\n",
    "        add_special_tokens = True,\n",
    "        return_token_type_ids=False,\n",
    "    )\n",
    "\n",
    "    input_ids = tokenized_senteneces.input_ids\n",
    "    attention_mask = tokenized_senteneces.attention_mask\n",
    "    label = torch.tensor(dataset['Class'])\n",
    "    \n",
    "    return {\n",
    "        \"label\": label,\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "    }\n",
    "\n",
    "def get_dataloader(train_path, test_path, tokenizer, MAX_LEN, BATCH_SIZE):\n",
    "    # 데이터 파일 로드\n",
    "    train_data, test_data = load_data(train_path, test_path)\n",
    "    \n",
    "    # 토크나이징 & 데이터셋 로드\n",
    "    tokenized_train = NewsDataset(preprocessing_data(train_data, tokenizer, MAX_LEN))\n",
    "    tokenized_test = NewsDataset(preprocessing_data(test_data, tokenizer, MAX_LEN))\n",
    "    \n",
    "    # train, valid split\n",
    "    generator = torch.Generator().manual_seed(RANDOM_SEED)\n",
    "    train, valid = random_split(tokenized_train, [0.8, 0.2], generator=generator)\n",
    "    \n",
    "    # DataLoader\n",
    "    train_dataloader = DataLoader(train, sampler=RandomSampler(train), batch_size=BATCH_SIZE, num_workers=8)\n",
    "    valid_dataloader = DataLoader(valid, sampler=SequentialSampler(valid), batch_size=BATCH_SIZE, num_workers=8)\n",
    "    test_dataloader = DataLoader(tokenized_test, sampler=SequentialSampler(tokenized_test), batch_size=BATCH_SIZE, num_workers=8)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d59d1ff4-76ef-4c25-b0ce-1b87647baccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from collections import OrderedDict\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e2fb6dc1-cbc6-4a9f-9653-6e6d1405fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, MODEL_NAME):\n",
    "        super(Model, self).__init__()\n",
    "        model_config = AutoConfig.from_pretrained(MODEL_NAME)\n",
    "        model_config.num_labels = 2\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, config=model_config)\n",
    "        self.model = model\n",
    "\n",
    "        self.accuracy = torchmetrics.classification.BinaryAccuracy()\n",
    "        self.precision = torchmetrics.classification.BinaryPrecision()\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "                self.model.parameters(),\n",
    "                lr=2e-5,\n",
    "                )\n",
    "        return optimizer\n",
    "\n",
    "    def _shared_step(self, batch, batch_idx):\n",
    "        labels = batch[\"label\"]\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "        output = self.model(\n",
    "                input_ids,\n",
    "                token_type_ids=None,\n",
    "                attention_mask=attention_mask,\n",
    "                labels=labels\n",
    "                )\n",
    "        return {'loss':output['loss'], \n",
    "                'labels':labels, \n",
    "                'logits': output['logits']\n",
    "                }\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        output = self._shared_step(batch, batch_idx)\n",
    "\n",
    "        train_loss = output['loss']\n",
    "        train_acc = self.accuracy(torch.argmax(output['logits'], dim=1), output['labels'])\n",
    "        train_precision = self.precision(torch.argmax(output['logits'], dim=1), output['labels'])\n",
    "\n",
    "        self.log('train_loss', train_loss)\n",
    "        self.log('train_acc_step', train_acc)\n",
    "        self.log('train_precision_step', train_precision)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        output = self._shared_step(batch, batch_idx)\n",
    "\n",
    "        valid_loss = output['loss']\n",
    "        valid_acc = self.accuracy(torch.argmax(output['logits'], dim=1), output['labels'])\n",
    "        valid_precision = self.precision(torch.argmax(output['logits'], dim=1), output['labels'])\n",
    "\n",
    "        self.log('valid_loss', valid_loss)\n",
    "        self.log('valid_acc_step', valid_acc)\n",
    "        self.log('valid_precision_step', valid_precision)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        output = self._shared_step(batch, batch_idx)\n",
    "        \n",
    "        test_acc = self.accuracy(torch.argmax(output['logits'], dim=1), output['labels'])\n",
    "        test_precision = self.precision(torch.argmax(output['logits'], dim=1), output['labels'])\n",
    "\n",
    "        self.log('test_acc_step', test_acc)\n",
    "        self.log('test_precision_step', test_precision)\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d187791-b8ac-4ade-bea0-69ad086d96c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = Model('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "2605ec4f-ae02-4582-8209-31580b840513",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "MAX_LEN = 450\n",
    "BATCH_SIZE = 8\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "f13b1ac0-b52a-4470-add5-980b65d043ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader, test_dataloader = get_dataloader('data_EC_00.csv', 'VL_data_EC.csv', tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3196a9cf-489d-4d48-9690-175570d126ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "51a1c3d3-de36-4202-8441-92e2ad7bdc75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b0d1c79c-9ab7-47ee-9230-7f1d02e731eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1aaf1a35-cb0b-4784-9925-e607b756c78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/aiffelmahimahi/anaconda3/envs/mahimahi/lib/python3.10/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:617: UserWarning: Checkpoint directory /home/aiffelmahimahi/model/checkpoint exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type                          | Params\n",
      "------------------------------------------------------------\n",
      "0 | model     | BertForSequenceClassification | 177 M \n",
      "1 | accuracy  | BinaryAccuracy                | 0     \n",
      "2 | precision | BinaryPrecision               | 0     \n",
      "------------------------------------------------------------\n",
      "177 M     Trainable params\n",
      "0         Non-trainable params\n",
      "177 M     Total params\n",
      "711.420   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2b072a9eef348a7b33dd38347ff638f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved. New best score: 0.678\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.677\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric valid_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.676\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric valid_loss did not improve in the last 2 records. Best score: 0.676. Signaling Trainer to stop.\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(project=\"mahimahi\")\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='valid_loss',\n",
    "    patience=2,\n",
    "    verbose=True,\n",
    "    mode='min'\n",
    ")\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    dirpath='model/checkpoint',\n",
    ")\n",
    "\n",
    "callbacks = [early_stop_callback, model_checkpoint]\n",
    "trainer = pl.Trainer(logger=wandb_logger,\n",
    "                     check_val_every_n_epoch=1,\n",
    "                     callbacks = callbacks,\n",
    "                     max_epochs=10)\n",
    "trainer.fit(model, train_dataloader, valid_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "32d47849-320c-4e1b-949e-83ce2141dfae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e21ae6c1a8884e999857f918c12243d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">       test_acc_step       </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4972088634967804     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">    test_precision_step    </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.4972088634967804     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m      test_acc_step      \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4972088634967804    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m   test_precision_step   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.4972088634967804    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'test_acc_step': 0.4972088634967804,\n",
       "  'test_precision_step': 0.4972088634967804}]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ca4836-81c3-451c-80f0-0883ef2c6859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c201268d-f491-4fa4-84cf-b0dcb6777665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
